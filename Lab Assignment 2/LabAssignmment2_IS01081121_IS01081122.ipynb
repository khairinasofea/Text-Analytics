{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60ecf0d",
   "metadata": {},
   "source": [
    "# Lab Assignment 2 - Text Analytics CISB5123\n",
    "\n",
    "In this lab assignment, we are going to do Sentiment Analysis where it will explore sentiment classification using the Amazon\n",
    "Fine Food Review dataset. (https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?select=Reviews.csv)\n",
    "\n",
    "#### Team members:\n",
    "1. Nur Husnina Binti Norishak (IS01081121)\n",
    "2. Nur Khairina Sofea Binti Khaidzir (IS01081122)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880f377",
   "metadata": {},
   "source": [
    "### Step 1: Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2daaf666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d372b8",
   "metadata": {},
   "source": [
    "### Step 2: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3770aa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>A1D3F6UI1RTXO0</td>\n",
       "      <td>Swopes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331856000</td>\n",
       "      <td>Hot &amp; Flavorful</td>\n",
       "      <td>BLACK MARKET HOT SAUCE IS WONDERFUL.... My hus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>AF50D40Y85TV3</td>\n",
       "      <td>Mike A.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1328140800</td>\n",
       "      <td>Great Hot Sauce and people who run it!</td>\n",
       "      <td>Man what can i say, this salsa is the bomb!! i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>A3G313KLWDG3PW</td>\n",
       "      <td>kefka82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1324252800</td>\n",
       "      <td>this sauce is the shiznit</td>\n",
       "      <td>this sauce is so good with just about anything...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>A3NIDDT7E7JIFW</td>\n",
       "      <td>V. B. Brookshaw</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1336089600</td>\n",
       "      <td>Not Hot</td>\n",
       "      <td>Not hot at all. Like the other low star review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>A132DJVI37RB4X</td>\n",
       "      <td>Scottdrum</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1332374400</td>\n",
       "      <td>Not hot, not habanero</td>\n",
       "      <td>I have to admit, I was a sucker for the large ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId                      ProfileName  \\\n",
       "0       1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1       2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2       3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3       4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4       5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "..    ...         ...             ...                              ...   \n",
       "995   996  B006F2NYI2  A1D3F6UI1RTXO0                           Swopes   \n",
       "996   997  B006F2NYI2   AF50D40Y85TV3                          Mike A.   \n",
       "997   998  B006F2NYI2  A3G313KLWDG3PW                          kefka82   \n",
       "998   999  B006F2NYI2  A3NIDDT7E7JIFW                  V. B. Brookshaw   \n",
       "999  1000  B006F2NYI2  A132DJVI37RB4X                        Scottdrum   \n",
       "\n",
       "     HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                       1                       1      5  1303862400   \n",
       "1                       0                       0      1  1346976000   \n",
       "2                       1                       1      4  1219017600   \n",
       "3                       3                       3      2  1307923200   \n",
       "4                       0                       0      5  1350777600   \n",
       "..                    ...                     ...    ...         ...   \n",
       "995                     1                       1      5  1331856000   \n",
       "996                     1                       1      5  1328140800   \n",
       "997                     1                       1      5  1324252800   \n",
       "998                     1                       2      1  1336089600   \n",
       "999                     2                       5      2  1332374400   \n",
       "\n",
       "                                    Summary  \\\n",
       "0                     Good Quality Dog Food   \n",
       "1                         Not as Advertised   \n",
       "2                     \"Delight\" says it all   \n",
       "3                            Cough Medicine   \n",
       "4                               Great taffy   \n",
       "..                                      ...   \n",
       "995                         Hot & Flavorful   \n",
       "996  Great Hot Sauce and people who run it!   \n",
       "997               this sauce is the shiznit   \n",
       "998                                 Not Hot   \n",
       "999                   Not hot, not habanero   \n",
       "\n",
       "                                                  Text  \n",
       "0    I have bought several of the Vitality canned d...  \n",
       "1    Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2    This is a confection that has been around a fe...  \n",
       "3    If you are looking for the secret ingredient i...  \n",
       "4    Great taffy at a great price.  There was a wid...  \n",
       "..                                                 ...  \n",
       "995  BLACK MARKET HOT SAUCE IS WONDERFUL.... My hus...  \n",
       "996  Man what can i say, this salsa is the bomb!! i...  \n",
       "997  this sauce is so good with just about anything...  \n",
       "998  Not hot at all. Like the other low star review...  \n",
       "999  I have to admit, I was a sucker for the large ...  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# read data - first 1000 rows\n",
    "data = pd.read_csv('Reviews.csv', nrows=1000)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55dff5f",
   "metadata": {},
   "source": [
    "### Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379a04f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check no. of rows & columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6effe802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>great taffy at a great price there was a wide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>A1D3F6UI1RTXO0</td>\n",
       "      <td>Swopes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331856000</td>\n",
       "      <td>Hot &amp; Flavorful</td>\n",
       "      <td>black market hot sauce is wonderful my husband...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>AF50D40Y85TV3</td>\n",
       "      <td>Mike A.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1328140800</td>\n",
       "      <td>Great Hot Sauce and people who run it!</td>\n",
       "      <td>man what can i say this salsa is the bomb i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>A3G313KLWDG3PW</td>\n",
       "      <td>kefka82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1324252800</td>\n",
       "      <td>this sauce is the shiznit</td>\n",
       "      <td>this sauce is so good with just about anything...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>A3NIDDT7E7JIFW</td>\n",
       "      <td>V. B. Brookshaw</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1336089600</td>\n",
       "      <td>Not Hot</td>\n",
       "      <td>not hot at all like the other low star reviewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>B006F2NYI2</td>\n",
       "      <td>A132DJVI37RB4X</td>\n",
       "      <td>Scottdrum</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1332374400</td>\n",
       "      <td>Not hot, not habanero</td>\n",
       "      <td>i have to admit i was a sucker for the large q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId                      ProfileName  \\\n",
       "0       1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1       2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2       3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3       4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4       5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "..    ...         ...             ...                              ...   \n",
       "995   996  B006F2NYI2  A1D3F6UI1RTXO0                           Swopes   \n",
       "996   997  B006F2NYI2   AF50D40Y85TV3                          Mike A.   \n",
       "997   998  B006F2NYI2  A3G313KLWDG3PW                          kefka82   \n",
       "998   999  B006F2NYI2  A3NIDDT7E7JIFW                  V. B. Brookshaw   \n",
       "999  1000  B006F2NYI2  A132DJVI37RB4X                        Scottdrum   \n",
       "\n",
       "     HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                       1                       1      5  1303862400   \n",
       "1                       0                       0      1  1346976000   \n",
       "2                       1                       1      4  1219017600   \n",
       "3                       3                       3      2  1307923200   \n",
       "4                       0                       0      5  1350777600   \n",
       "..                    ...                     ...    ...         ...   \n",
       "995                     1                       1      5  1331856000   \n",
       "996                     1                       1      5  1328140800   \n",
       "997                     1                       1      5  1324252800   \n",
       "998                     1                       2      1  1336089600   \n",
       "999                     2                       5      2  1332374400   \n",
       "\n",
       "                                    Summary  \\\n",
       "0                     Good Quality Dog Food   \n",
       "1                         Not as Advertised   \n",
       "2                     \"Delight\" says it all   \n",
       "3                            Cough Medicine   \n",
       "4                               Great taffy   \n",
       "..                                      ...   \n",
       "995                         Hot & Flavorful   \n",
       "996  Great Hot Sauce and people who run it!   \n",
       "997               this sauce is the shiznit   \n",
       "998                                 Not Hot   \n",
       "999                   Not hot, not habanero   \n",
       "\n",
       "                                                  Text  \n",
       "0    i have bought several of the vitality canned d...  \n",
       "1    product arrived labeled as jumbo salted peanut...  \n",
       "2    this is a confection that has been around a fe...  \n",
       "3    if you are looking for the secret ingredient i...  \n",
       "4    great taffy at a great price there was a wide ...  \n",
       "..                                                 ...  \n",
       "995  black market hot sauce is wonderful my husband...  \n",
       "996  man what can i say this salsa is the bomb i ha...  \n",
       "997  this sauce is so good with just about anything...  \n",
       "998  not hot at all like the other low star reviewe...  \n",
       "999  i have to admit i was a sucker for the large q...  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing HTML tags and Unwanted characters\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply the clean_text function to the 'Text' column\n",
    "data['Text'] = data['Text'].apply(clean_text)\n",
    "\n",
    "# View the cleaned text data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51395571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                        0\n",
       "ProductId                 0\n",
       "UserId                    0\n",
       "ProfileName               0\n",
       "HelpfulnessNumerator      0\n",
       "HelpfulnessDenominator    0\n",
       "Score                     0\n",
       "Time                      0\n",
       "Summary                   0\n",
       "Text                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f580415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for sentiment analysis\n",
    "data = data[['Score', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69e4a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nurhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\nurhu\\AppData\\Local\\Temp\\ipykernel_11944\\1095417300.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Tokens'] = data['Text'].apply(lambda x: nltk.word_tokenize(x))\n"
     ]
    }
   ],
   "source": [
    "# tokenization text\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "data['Tokens'] = data['Text'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6d945da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nurhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\nurhu\\AppData\\Local\\Temp\\ipykernel_11944\\3342019190.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Tokens'] = data['Tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
     ]
    }
   ],
   "source": [
    "# Stop words\n",
    "# Download the WordNet lemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens\n",
    "data['Tokens'] = data['Tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "327f101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurhu\\AppData\\Local\\Temp\\ipykernel_11944\\100748633.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Preprocessed_Text'] = data['Tokens'].apply(lambda x: ' '.join(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Preprocessed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "      <td>[i, have, bought, several, of, the, vitality, ...</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "      <td>[product, arrived, labeled, a, jumbo, salted, ...</td>\n",
       "      <td>product arrived labeled a jumbo salted peanuts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "      <td>[this, is, a, confection, that, ha, been, arou...</td>\n",
       "      <td>this is a confection that ha been around a few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "      <td>[if, you, are, looking, for, the, secret, ingr...</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>great taffy at a great price there was a wide ...</td>\n",
       "      <td>[great, taffy, at, a, great, price, there, wa,...</td>\n",
       "      <td>great taffy at a great price there wa a wide a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>5</td>\n",
       "      <td>black market hot sauce is wonderful my husband...</td>\n",
       "      <td>[black, market, hot, sauce, is, wonderful, my,...</td>\n",
       "      <td>black market hot sauce is wonderful my husband...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5</td>\n",
       "      <td>man what can i say this salsa is the bomb i ha...</td>\n",
       "      <td>[man, what, can, i, say, this, salsa, is, the,...</td>\n",
       "      <td>man what can i say this salsa is the bomb i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>5</td>\n",
       "      <td>this sauce is so good with just about anything...</td>\n",
       "      <td>[this, sauce, is, so, good, with, just, about,...</td>\n",
       "      <td>this sauce is so good with just about anything...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "      <td>not hot at all like the other low star reviewe...</td>\n",
       "      <td>[not, hot, at, all, like, the, other, low, sta...</td>\n",
       "      <td>not hot at all like the other low star reviewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2</td>\n",
       "      <td>i have to admit i was a sucker for the large q...</td>\n",
       "      <td>[i, have, to, admit, i, wa, a, sucker, for, th...</td>\n",
       "      <td>i have to admit i wa a sucker for the large qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Score                                               Text  \\\n",
       "0        5  i have bought several of the vitality canned d...   \n",
       "1        1  product arrived labeled as jumbo salted peanut...   \n",
       "2        4  this is a confection that has been around a fe...   \n",
       "3        2  if you are looking for the secret ingredient i...   \n",
       "4        5  great taffy at a great price there was a wide ...   \n",
       "..     ...                                                ...   \n",
       "995      5  black market hot sauce is wonderful my husband...   \n",
       "996      5  man what can i say this salsa is the bomb i ha...   \n",
       "997      5  this sauce is so good with just about anything...   \n",
       "998      1  not hot at all like the other low star reviewe...   \n",
       "999      2  i have to admit i was a sucker for the large q...   \n",
       "\n",
       "                                                Tokens  \\\n",
       "0    [i, have, bought, several, of, the, vitality, ...   \n",
       "1    [product, arrived, labeled, a, jumbo, salted, ...   \n",
       "2    [this, is, a, confection, that, ha, been, arou...   \n",
       "3    [if, you, are, looking, for, the, secret, ingr...   \n",
       "4    [great, taffy, at, a, great, price, there, wa,...   \n",
       "..                                                 ...   \n",
       "995  [black, market, hot, sauce, is, wonderful, my,...   \n",
       "996  [man, what, can, i, say, this, salsa, is, the,...   \n",
       "997  [this, sauce, is, so, good, with, just, about,...   \n",
       "998  [not, hot, at, all, like, the, other, low, sta...   \n",
       "999  [i, have, to, admit, i, wa, a, sucker, for, th...   \n",
       "\n",
       "                                     Preprocessed_Text  \n",
       "0    i have bought several of the vitality canned d...  \n",
       "1    product arrived labeled a jumbo salted peanuts...  \n",
       "2    this is a confection that ha been around a few...  \n",
       "3    if you are looking for the secret ingredient i...  \n",
       "4    great taffy at a great price there wa a wide a...  \n",
       "..                                                 ...  \n",
       "995  black market hot sauce is wonderful my husband...  \n",
       "996  man what can i say this salsa is the bomb i ha...  \n",
       "997  this sauce is so good with just about anything...  \n",
       "998  not hot at all like the other low star reviewe...  \n",
       "999  i have to admit i wa a sucker for the large qu...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the tokens back into sentences\n",
    "data['Preprocessed_Text'] = data['Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "data.to_csv('extracted_data_1000reviews.csv', index=False)\n",
    "\n",
    "# Preview the preprocessed data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b7465",
   "metadata": {},
   "source": [
    "### Step 4: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7514cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff98ce6",
   "metadata": {},
   "source": [
    "#### 1. Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e567614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW feature shape: (1000, 6047)\n",
      "Vocabulary size: 6047\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "bow_features = vectorizer.fit_transform(data['Preprocessed_Text'])\n",
    "\n",
    "# Get the vocabulary (unique words)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the shape of the BoW features and the vocabulary size\n",
    "print(\"BoW feature shape:\", bow_features.shape)\n",
    "print(\"Vocabulary size:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e13f5",
   "metadata": {},
   "source": [
    "#### 2. Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c879b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF feature shape: (1000, 6047)\n",
      "Vocabulary size: 6047\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['Preprocessed_Text'])\n",
    "\n",
    "# Get the vocabulary (unique words)\n",
    "tfidf_vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the shape of the TF-IDF features and the vocabulary size\n",
    "print(\"TF-IDF feature shape:\", tfidf_features.shape)\n",
    "print(\"Vocabulary size:\", len(tfidf_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6100c7",
   "metadata": {},
   "source": [
    "### Step 5: Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f39d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70b2b8",
   "metadata": {},
   "source": [
    "#### 1. Lexicon-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d682071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\nurhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "C:\\Users\\nurhu\\AppData\\Local\\Temp\\ipykernel_11944\\362007389.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Sentiment'] = data['Score'].apply(assign_sentiment)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon-based Approach Accuracy: 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurhu\\AppData\\Local\\Temp\\ipykernel_11944\\362007389.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Lexicon_Sentiment'] = data['Preprocessed_Text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
      "C:\\Users\\nurhu\\AppData\\Local\\Temp\\ipykernel_11944\\362007389.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Lexicon_Sentiment_Label'] = data['Lexicon_Sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))\n"
     ]
    }
   ],
   "source": [
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Assign sentiment labels based on the 'Score' column\n",
    "def assign_sentiment(score):\n",
    "    if score >= 4:\n",
    "        return 'Positive'\n",
    "    elif score <= 2:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data['Sentiment'] = data['Score'].apply(assign_sentiment)\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores for each review\n",
    "data['Lexicon_Sentiment'] = data['Preprocessed_Text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "# Map sentiment scores to labels\n",
    "data['Lexicon_Sentiment_Label'] = data['Lexicon_Sentiment'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))\n",
    "\n",
    "# Evaluate the lexicon-based approach\n",
    "lexicon_accuracy = accuracy_score(data['Sentiment'], data['Lexicon_Sentiment_Label'])\n",
    "print(\"Lexicon-based Approach Accuracy:\", lexicon_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05723a2",
   "metadata": {},
   "source": [
    "#### 2. Machine learning-based approaches (Naive Bayes Accuracy & SVM Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "048109e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        25\n",
      "     Neutral       0.00      0.00      0.00        11\n",
      "    Positive       0.82      1.00      0.90       164\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.27      0.33      0.30       200\n",
      "weighted avg       0.67      0.82      0.74       200\n",
      "\n",
      "SVM Accuracy: 0.845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.32      0.44        25\n",
      "     Neutral       0.00      0.00      0.00        11\n",
      "    Positive       0.85      0.98      0.91       164\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.53      0.43      0.45       200\n",
      "weighted avg       0.79      0.84      0.80       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurhu\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nurhu\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nurhu\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nurhu\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nurhu\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nurhu\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\nurhu\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Preprocessed_Text'], data['Sentiment'], test_size= 0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train and evaluate Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "nb_predictions = nb_classifier.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "print(classification_report(y_test, nb_predictions))\n",
    "\n",
    "# Train and evaluate SVM classifier\n",
    "svm_classifier = LinearSVC()\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test_tfidf)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a71eb",
   "metadata": {},
   "source": [
    "### Discussion - strengths and weaknesses of the selected models for sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c60e9",
   "metadata": {},
   "source": [
    "### Lexicon-Based Approach\n",
    "Results Overview:\n",
    "- Accuracy: 0.807\n",
    "\n",
    "Strengths:\n",
    "- Simplicity and Speed: straightforward to implement and fast to apply, as it doesn't require model training.\n",
    "- High accuracy and effectiveness in identifying positive sentiments\n",
    "\n",
    "Weaknesses:\n",
    "- Fixed Lexicon: Might not capture context or nuances well due to the fixed nature of sentiment scores assigned to words.\n",
    "- Potential Bias: If the lexicon is not comprehensive or up-to-date, it may not accurately reflect the nuances of language used in reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd8e70",
   "metadata": {},
   "source": [
    "### Naive Bayes \n",
    "Results Overview:\n",
    "- Accuracy: 0.82\n",
    "\n",
    "Strengths:\n",
    "- Simple and fast, good for a baseline model.\n",
    "- High accuracy and effectiveness in identifying positive sentiments.\n",
    "\n",
    "Weaknesses:\n",
    "- Inability to classify negative and neutral reviews suggests it struggles with class imbalance and more nuanced sentiment expressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff303f8",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "Results Overview:\n",
    "- Accuracy: 0.845\n",
    "\n",
    "Strengths:\n",
    "- Highest accuracy among the models.\n",
    "- Shows capability in distinguishing between positive and negative sentiments to some extent.\n",
    "\n",
    "Weaknesses:\n",
    "- Requires more computational resources.\n",
    "- Still lacks effectiveness in classifying neutral reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c988e",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "- Since we only analyze 1000 rows from this data set due to uncapable of our hardware, SVM will be the most suitable choice given its superior overall accuracy and its ability to classify negative reviews better than the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d7798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
