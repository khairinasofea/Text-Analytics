{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c313714",
   "metadata": {},
   "source": [
    "# Lab Assignment 3\n",
    "\n",
    "1. Nur Husnina Binti Norishak (IS01081121)\n",
    "2. Nur Khairina Sofea Binti Khaidzir (IS01081122)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017abf1",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f4c73cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# For topic modeling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48259409",
   "metadata": {},
   "source": [
    "### Read the data (use only the ‘text’ column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "552a3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'news_dataset.csv'\n",
    "news_data = pd.read_csv(file_path)\n",
    "\n",
    "# Use only the 'text' column\n",
    "texts = news_data['text'].dropna().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4b88d",
   "metadata": {},
   "source": [
    "### Perform text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78c62f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wonder', 'anyon', 'could', 'enlighten', 'car', 'saw', 'day', 'sport', 'car', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'realli', 'small', 'addit', 'front', 'bumper', 'separ', 'rest', 'bodi', 'know', 'anyon', 'tellm', 'model', 'name', 'engin', 'spec', 'year', 'product', 'car', 'made', 'histori', 'whatev', 'info', 'funki', 'look', 'car', 'plea']\n"
     ]
    }
   ],
   "source": [
    "# Initialize stop words, stemmer, and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Filter out non-alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token.isalnum()] \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "     # Stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the texts\n",
    "processed_texts = [preprocess_text(text) for text in texts]\n",
    "print(processed_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2670a1d2",
   "metadata": {},
   "source": [
    "### Perform LDA using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e2cd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a gensim dictionary object from the preprocessed doc\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "\n",
    "#convert each preprocessed document into a BoW representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f89e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run LDA\n",
    "lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b16503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list to store dominiant topic labels for each doc\n",
    "article_labels = []\n",
    "\n",
    "#iterate over each processed_texts\n",
    "for i, doc in enumerate(processed_texts):\n",
    "    #for each doc, convert to box representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    \n",
    "    #get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    \n",
    "    #determine the topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    \n",
    "    #append to the list\n",
    "    article_labels.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2a316",
   "metadata": {},
   "source": [
    "### Interpret the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "533cee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                                 Article  Topic\n",
      "0      I was wondering if anyone out there could enli...      3\n",
      "1      I recently posted an article asking what kind ...      3\n",
      "2      \\nIt depends on your priorities.  A lot of peo...      3\n",
      "3      an excellent automatic can be found in the sub...      3\n",
      "4      : Ford and his automobile.  I need information...      3\n",
      "...                                                  ...    ...\n",
      "11091  Secrecy in Clipper Chip\\n\\nThe serial number o...      0\n",
      "11092  Hi !\\n\\nI am interested in the source of FEAL ...      0\n",
      "11093  The actual algorithm is classified, however, t...      0\n",
      "11094  \\n\\tThis appears to be generic calling upon th...      3\n",
      "11095  \\nProbably keep quiet and take it, lest they g...      3\n",
      "\n",
      "[11096 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create dataframe\n",
    "df_result = pd.DataFrame({\"Article\":texts, \"Topic\":article_labels})\n",
    "\n",
    "#print dataframe\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df_result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae13cfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for Topic #0:\n",
      "['use', 'key', 'file', 'system', 'encrypt', 'one', 'program', 'get', 'chip', 'would']\n",
      "\n",
      "Top terms for Topic #1:\n",
      "['q', 'x', 'max', 'g', 'r', 'p', 'n', 'db', 'k', 'c']\n",
      "\n",
      "Top terms for Topic #2:\n",
      "['game', 'team', 'year', 'play', 'new', 'player', 'win', 'season', 'first', 'leagu']\n",
      "\n",
      "Top terms for Topic #3:\n",
      "['would', 'peopl', 'one', 'think', 'say', 'like', 'know', 'go', 'get', 'time']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print top terms for each topic\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    print(f\"Top terms for Topic #{topic_id}:\")\n",
    "    top_terms = lda_model.show_topic(topic_id, topn=10)\n",
    "    print([term[0] for term in top_terms])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "deca0284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"use\" (weight: 0.016)\n",
      "- \"key\" (weight: 0.009)\n",
      "- \"file\" (weight: 0.007)\n",
      "- \"system\" (weight: 0.007)\n",
      "- \"encrypt\" (weight: 0.006)\n",
      "- \"one\" (weight: 0.006)\n",
      "- \"program\" (weight: 0.005)\n",
      "- \"get\" (weight: 0.005)\n",
      "- \"chip\" (weight: 0.005)\n",
      "- \"would\" (weight: 0.005)\n",
      "\n",
      "Topic 1:\n",
      "- \"q\" (weight: 0.055)\n",
      "- \"x\" (weight: 0.053)\n",
      "- \"max\" (weight: 0.048)\n",
      "- \"g\" (weight: 0.031)\n",
      "- \"r\" (weight: 0.031)\n",
      "- \"p\" (weight: 0.026)\n",
      "- \"n\" (weight: 0.023)\n",
      "- \"db\" (weight: 0.023)\n",
      "- \"k\" (weight: 0.017)\n",
      "- \"c\" (weight: 0.017)\n",
      "\n",
      "Topic 2:\n",
      "- \"game\" (weight: 0.009)\n",
      "- \"team\" (weight: 0.008)\n",
      "- \"year\" (weight: 0.007)\n",
      "- \"play\" (weight: 0.006)\n",
      "- \"new\" (weight: 0.005)\n",
      "- \"player\" (weight: 0.005)\n",
      "- \"win\" (weight: 0.003)\n",
      "- \"season\" (weight: 0.003)\n",
      "- \"first\" (weight: 0.003)\n",
      "- \"leagu\" (weight: 0.003)\n",
      "\n",
      "Topic 3:\n",
      "- \"would\" (weight: 0.009)\n",
      "- \"peopl\" (weight: 0.008)\n",
      "- \"one\" (weight: 0.008)\n",
      "- \"think\" (weight: 0.006)\n",
      "- \"say\" (weight: 0.005)\n",
      "- \"like\" (weight: 0.005)\n",
      "- \"know\" (weight: 0.005)\n",
      "- \"go\" (weight: 0.005)\n",
      "- \"get\" (weight: 0.004)\n",
      "- \"time\" (weight: 0.004)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the top terms for each topic with weight\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    \n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fcd0a",
   "metadata": {},
   "source": [
    "### Evaluate the LDA model using Coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e457d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence Score (C_V): 0.6368\n"
     ]
    }
   ],
   "source": [
    "# Calculate the coherence score for the LDA model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print(f'Topic Coherence Score (C_V): {coherence_lda:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb68e5",
   "metadata": {},
   "source": [
    "###### Notes\n",
    "\n",
    "Topic 0: Computer Program |\n",
    "Topic 1: Variable |\n",
    "Topic 2: E-sport Game |\n",
    "Topic 3: Opinions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ef475",
   "metadata": {},
   "source": [
    "###### A score of 0.6368 means the groups is good at having similar group together"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
